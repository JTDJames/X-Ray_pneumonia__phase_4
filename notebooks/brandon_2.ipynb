{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Chest X-Ray Image Classification of Pneumonia Diagnoses using Deep Learning Modeling CNNs </h1>\n",
    "\n",
    "Authors: Ilan Haskel, Justin James, Roshni Janakiraman, Leif Schultz, and Brandon Sienkiewicz\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(72)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "**Stakeholder:** Children's Hospital - Board of Directors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case \n",
    "\n",
    "Pneumonia is the primary cause of childhood hospitalization [[1]](https://pubmed.ncbi.nlm.nih.gov/25695124/). Pediatric pneumonia is lethal without proper treatment, accounting for 15% of all childhood deaths [[2]](https://www.who.int/en/news-room/fact-sheets/detail/pneumonia).  As the disease progresses, pediatric pneumonia requires a significant amount of hospital resources for treatment and poses a high cost of care for patients [[3]](https://www.sciencedirect.com/science/article/pii/S2352646719300274). Therefore, timely and accurate diagnosis of pneumonia is critical for successful treatment.\n",
    "\n",
    "There are many complications to quickly and accurately diagnosing pneumonia. To determine the right treatment protocol, doctors need to determine whether pneumonia is *bacterial* or *viral* [[4]](https://www.nejm.org/doi/full/10.1056/NEJMoa1405870). Clinical features alone are not sufficient to accurately diagnose pneumonia [[5]](https://pneumonia.biomedcentral.com/articles/10.15172/pneu.2014.5/464#Sec4).\n",
    "\n",
    "Chest X-Ray evaluation is the current gold standard for diagnosing pneumonia [[6]](https://academic.oup.com/cid/article/31/2/347/293404). However, this method has key limitations. Chest X-ray interpretation is labor intensive and prone to human error; there is a shortage of radiologists with sufficient training to read chest X-rays[[7]](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(21)00106-0/fulltext). Even for experienced radiologists, reliability and accuracy scores range from 38-76% [[8]](https://www.ajronline.org/doi/10.2214/AJR.19.21521).\n",
    "\n",
    "Artificial Intelligence can improve the process of diagnosing pneumonia accurately and efficiently. Computer aided diagnostic systems have shown reasonable accuracy in detecting infections from X-rays. When aided by AI, radiologists are significantly more accurate at diagnosing pneumonia compared to unassisted[[9]](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(21)00106-0/fulltext). Computerized models can increase the overall efficiency of the diagnostic process by reducing work burden on radiologists. Quicker detection allows doctors to start treatment protocol sooner, which can reduce severity and duration of illness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of Current Project\n",
    "\n",
    "We sought to create and optimize image classification models that could diagnose pneumonia from chest X-ray images. Our specific goals for these models were to:\n",
    "\n",
    "1. Accurately distinguish pneumonia *positive* cases from *negative cases*\n",
    "2. Given a new chest X-Ray image, accurately classify case as *bacterial pneumonia,* *viral pneumonia* or *non-pneumonia*\n",
    "3. Minimize *false negative* diagnoses, given lethality of pneumonia without proper treatment\n",
    "4. Increase the efficiency of the chest X-ray diagnostic process by deploying a quick, simple-to-run testing system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "\n",
    "Before reading in the data, we import all relevant packages. Since our data consists of images, we utilize the `image_dataset_from_directory` from Tensorflow. In order to use this data for analysis, the data was instantiated as images and labels and then converted to a Numpy array as it is an easier data type to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5856 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='D:/Flatiron/X-Ray_pneumonia__phase_4/data',\n",
    "    batch_size=10000,\n",
    "    seed=72    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The data as given already contained a train, test split; however, the validation set did not appear to have any viral pneumonia photos in the validation set. The validation set was also very small, containing only sixteen total photos. In order to accurately test on unseen data, we combined the training, testing, and validation datasets and then conducted a train, test split to recreate each data set. We chose to do a split of 80% train, 10% test, and 10% validation as it appears to be a generally acceptable proportion for a train test split. This new split allows for more meaningful evaluation when trying to accurately predict bacterial pneumonia, viral pneumonia, and non-pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images,\n",
    "    labels,\n",
    "    random_state=42,\n",
    "    test_size=585\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    random_state=42,\n",
    "    test_size=585\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4686, 256, 256, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 256, 256, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 256, 256, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new train,test split, it appears that bacterial pneumonia is the dominate class, at roughly 47.5%, followed by the non-pneumonia class at roughly 26.5%, and finally the viral pneumonia class at roughly 26%. This data does not appear to be unbalanced and, therefore, should not need any upsampling. Since no upsampling is needed, the next step is to normalize the image matrices. Since our images are 256x256 pixels, we divide the entries by 255 to normalize. This process makes each entry's value, or pixel, between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.470124\n",
       "0    0.269953\n",
       "2    0.259923\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_labels).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, val_images = train_images/255, test_images/255, val_images/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we are evaluating a multiclass classification problem, we need to transform our labels with `OneHotEncoder`. This process splits the labels binary columns that state which class a given image is. Following this process, the data is ready for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "train_labels_encoded = ohe.fit_transform(train_labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "test_labels_encoded = ohe.fit_transform(test_labels.reshape(-1, 1)).toarray()\n",
    "\n",
    "val_labels_encoded = ohe.fit_transform(val_labels.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "\n",
    "Before starting the modelling, a function titled `evaluate` was created to visualize the results of each model. This function takes the model and the results and prints out the following visualizations: loss for the training and validation set, accuracy for the training and validation set, a confusion matrix for the multiclass problem, the accuracy for each class in the multiclass, a confusion matrix for the binary problem, and the accuracy for each class in the binary. The function is detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, results, final=False):\n",
    "    \n",
    "    #Create a function that provides useful vis for model\n",
    "    #performance. This is especially useful as we are most\n",
    "    #concerned with the number of false negatives\n",
    "    \n",
    "    if final:\n",
    "        val_label=\"test\"\n",
    "    else:\n",
    "        val_label=\"validation\"\n",
    "        \n",
    "\n",
    "    #Extracts metrics from the results of the model (model fitting)\n",
    "    \n",
    "    train_loss = results.history['loss']\n",
    "    val_loss = results.history['val_loss']\n",
    "    train_accuracy = results.history['accuracy']\n",
    "    val_accuracy = results.history['val_accuracy']\n",
    "\n",
    "    #Setting up the plots\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(20, 10))\n",
    "\n",
    "    # Plotting loss\n",
    "    ax1.set_title(\"Loss\")\n",
    "    sns.lineplot(x=results.epoch, y=train_loss, ax=ax1, label=\"train\")\n",
    "    sns.lineplot(x=results.epoch, y=val_loss, ax=ax1, label=val_label)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plotting accuracy\n",
    "    \n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    sns.lineplot(x=results.epoch, y=train_accuracy, ax=ax2, label=\"train\")\n",
    "    sns.lineplot(x=results.epoch, y=val_accuracy, ax=ax2, label=val_label)\n",
    "    ax2.legend()\n",
    "    \n",
    "    #Uses the model to make predictions and creates a confusion\n",
    "    #matrix for the multiclass\n",
    "    \n",
    "    y_pred = model.predict(test_images)\n",
    "    cm = confusion_matrix(test_labels, np.argmax(y_pred, axis=1))\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    \n",
    "    #Plotting the multiclass confusion matrix\n",
    "\n",
    "    sns.heatmap(cm, ax=ax3, annot=True, cmap='Blues', fmt='0.5g')\n",
    "    \n",
    "    #Setting up the barplot showing the accuracy of each class\n",
    "    #This involves creating labels and heights for the plot\n",
    "    #The heights are determined from the values in the confusion\n",
    "    #matrix\n",
    "\n",
    "    label = ['Healthy Accuracy',\n",
    "             'Bacterial Accuracy',\n",
    "             'Viral Accuracy']\n",
    "\n",
    "    height = [(cm_df[0][0]/sum(cm_df[0]))*100,\n",
    "              (cm_df[1][1]/sum(cm_df[1]))*100,\n",
    "              (cm_df[2][2]/sum(cm_df[2]))*100]\n",
    "    \n",
    "    #Plotting the class accuracy\n",
    "\n",
    "    ax4.bar(x=label, height=height)\n",
    "    plt.sca(ax4)\n",
    "    xlocs, xlabs = plt.xticks()\n",
    "    plt.ylim(top=100)\n",
    "    plt.ylabel('Accuracy Percentage')\n",
    "    plt.title('Model Accuracy')\n",
    "    for i, j in enumerate(height):\n",
    "        ax4.text(xlocs[i],\n",
    "                 j-30,\n",
    "                 ((str(round(j,1)))+'%'),\n",
    "                 ha ='center',\n",
    "                 bbox = dict(facecolor = 'white', alpha = .5))\n",
    "        \n",
    "    #Using the previous confusion matrix to create a binary\n",
    "    #Confusion matrix\n",
    "        \n",
    "    cm_simple = [[cm_df[0][0], cm_df[1][0]+cm_df[2][0]],\n",
    "                 [cm_df[0][1]+cm_df[0][2], cm_df[1][1]+cm_df[1][2]+cm_df[2][1]+cm_df[2][2]]]\n",
    "    cm_simple_df = pd.DataFrame(cm_simple)\n",
    "    \n",
    "    #Plotting the binary confusion matrix\n",
    "    \n",
    "    sns.heatmap(cm_simple, ax=ax5, annot=True, cmap='Blues', fmt='0.5g')\n",
    "    \n",
    "    #Setting up the barplot showing the accuracy of each class\n",
    "    #This involves creating labels and heights for the plot\n",
    "    #The heights are determined from the values in the confusion\n",
    "    #matrix\n",
    "    \n",
    "    simple_label = ['Healthy\\n Accuracy',\n",
    "                    'Pneumonia\\n Accuracy']\n",
    "    \n",
    "    simple_height = [(cm_simple_df[0][0]/sum(cm_simple_df[0]))*100,\n",
    "                     (cm_simple_df[1][1]/sum(cm_simple_df[1]))*100]\n",
    "    \n",
    "    #Plotting the class accuracy\n",
    "    \n",
    "    ax6.bar(x=simple_label, height=simple_height)\n",
    "    plt.sca(ax6)\n",
    "    xlocs, xlabs = plt.xticks()\n",
    "    plt.ylim(top=100)\n",
    "    plt.ylabel('Accuracy Percentage')\n",
    "    plt.title('Model Accuracy')\n",
    "    for k, l in enumerate(simple_height):\n",
    "        ax6.text(xlocs[k],\n",
    "                 l-30,\n",
    "                 ((str(round(l,1)))+'%'),\n",
    "                 ha ='center',\n",
    "                 bbox = dict(facecolor = 'white', alpha = .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model (Iteration One)\n",
    "\n",
    "For the baseline model, a sequential model was chosen. The parameters chosen were chosen in order to prevent the model from having too many nodes. The input layer is a 2D convolutional layer with a 2x2 convolutional window and a dimensionality of 64. The activation used is relu. Padding was added in order to accomodate more layers in the future. The first hidden layer is a 2D max pooling layer which downsamples the input. The next hidden layers are a 2D convolutional layer with a dimensionality of 32 followed by a 2D max pooling layer. These layers are repeated once more except the following 2D convolutional layer has a dimensionality of 16. A flattening layer is then added, followed by a dense layer with dimensionality of 500. This will likely cause overfitting; however, since this is a baseline model, we can tune this in future iterations. The output layer is a dense layer with dimensionality 3. The filter was set to 3 as there are 3 category outputs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = models.Sequential()\n",
    "\n",
    "baseline.add(layers.Conv2D(filters=64,\n",
    "                           kernel_size=2,\n",
    "                           padding=\"same\",\n",
    "                           activation=\"relu\",\n",
    "                           input_shape=(256,256,3)))\n",
    "\n",
    "baseline.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "baseline.add(layers.Conv2D(filters=32,\n",
    "                           kernel_size=2,\n",
    "                           padding=\"same\",\n",
    "                           activation =\"relu\"))\n",
    "baseline.add(layers.MaxPooling2D(pool_size=2))\n",
    "baseline.add(layers.Conv2D(filters=16,\n",
    "                           kernel_size=2,\n",
    "                           padding=\"same\",\n",
    "                           activation=\"relu\"))\n",
    "baseline.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "baseline.add(layers.Flatten())\n",
    "\n",
    "baseline.add(layers.Dense(500,activation=\"relu\"))\n",
    "\n",
    "baseline.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following model creating, the model was compiled using `adam` as the optimizer and `categorical_crossentropy` as the loss metric. `categorical_crossentropy` was chosen as `train_labels` is not a sparse matrix, but rather, a Numpy array. `accuracy` was chosen for the metric as it most closely fits our evaluation metric. The confusion matrices will be the best indicator of model performance as we are looking to minimize the false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 79s 2s/step - loss: 1.0950 - accuracy: 0.5849 - val_loss: 0.6480 - val_accuracy: 0.7436\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 78s 2s/step - loss: 0.5747 - accuracy: 0.7621 - val_loss: 0.5139 - val_accuracy: 0.7932\n",
      "Epoch 3/10\n",
      " 9/37 [======>.......................] - ETA: 52s - loss: 0.5261 - accuracy: 0.7691"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-33be8b4cd6ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  metrics=['accuracy'])\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m results = baseline.fit(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_labels_encoded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "results = baseline.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model completed with an accuracy of 88.3% on the training data and 81.2% on the validation data. This model is obviously overfit which is clearly shown in the top two visualizations below. The model also does a decent job of predicting each type of pneumonia, 76.2% accuracy when predicting bacterial pneumonia and 63.2% predicting viral pneumonia. It appears that there are only 18 false negatives in this model, meaning the false negative rate is roughly 10.6%. This is a good start for the baseline model but could be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Model Iteration\n",
    "\n",
    "For the second model iteration, the layers before the flattening layer were not changed. Instead of one dense layer following flattening, two dense layers with dimensionality 128 and 64 replace the layer. This should help to reduce overfitting. The model was compiled and fit in the same way as the `baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(filters=64,\n",
    "                         kernel_size=2,\n",
    "                         padding=\"same\",\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(256,256,3)))\n",
    "\n",
    "model2.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model2.add(layers.Conv2D(32,\n",
    "                         3,\n",
    "                         padding=\"same\",\n",
    "                         activation =\"relu\"))\n",
    "model2.add(layers.MaxPooling2D(pool_size=2))\n",
    "model2.add(layers.Conv2D(16,\n",
    "                         3,\n",
    "                         padding=\"same\",\n",
    "                         activation=\"relu\"))\n",
    "model2.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "\n",
    "model2.add(layers.Dense(128,activation=\"relu\"))\n",
    "model2.add(layers.Dense(64,activation=\"relu\"))\n",
    "model2.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model2.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the second model does appear to reduce overfitting and more accurately predict bacterial and viral pneumonia, it has a higher false negative rate than our baseline model, 11.9% versus 10.6%. Since we are trying to minimize the false negatives, this model overall appears to perform worse than our baseline. However, the changes to the dense layers appears to have reduced overfitting. Hence, these layers will be kept going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model2, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Model Iteration\n",
    "\n",
    "The third model adds a few layers and parameters. First, L2 regularization is added to the initial hidden 2D convolutional layers with a regularization factor of 0.05. Then 2D convolutional layers are added between the previous 2D convolutional layers and the max pooling layers. These have the same set-up as their previous layers with dimensionality 16 and 8 respectively. Also, in order to further reduce overfitting, the two previous dense layers after flattening were replaced with three dense layers of dimensionality 64, 32, and 16. The model was compiled and fit in the same way as the `baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "\n",
    "model3.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=3,\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(256,256,3)))\n",
    "\n",
    "model3.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model3.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model3.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\"))\n",
    "model3.add(layers.MaxPooling2D(pool_size=2))\n",
    "model3.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model3.add(layers.Conv2D(filters=8,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\"))\n",
    "model3.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "\n",
    "model3.add(layers.Dense(64,activation=\"relu\"))\n",
    "model3.add(layers.Dense(32,activation=\"relu\"))\n",
    "model3.add(layers.Dense(16,activation=\"relu\"))\n",
    "model3.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model3.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, this model greatly reduced overfitting and had a false negative rate of 7.5% which is a vast improvement from the previous models. This model does not appear to be overfit and also has a decent accuracy at predicting baterial and viral pneumonia, 75.7% and 62.8% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model3, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Model (Second Iteration)\n",
    "\n",
    "In order to try to improve this model, regularization was added to the two remaining hidden 2D convolutional layers and two 25% dropout layers were added. The model was compiled and fit in the same way as the `baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3b = models.Sequential()\n",
    "\n",
    "model3b.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=3,\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(256,256,3)))\n",
    "\n",
    "model3b.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model3b.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model3b.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model3b.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model3b.add(layers.Dropout(0.25))\n",
    "\n",
    "model3b.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model3b.add(layers.Conv2D(filters=8,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model3b.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model3b.add(layers.Dropout(0.25))\n",
    "\n",
    "model3b.add(layers.Flatten())\n",
    "\n",
    "model3b.add(layers.Dense(64,activation=\"relu\"))\n",
    "model3b.add(layers.Dense(32,activation=\"relu\"))\n",
    "model3b.add(layers.Dense(16,activation=\"relu\"))\n",
    "model3b.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3b.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model3b.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this model did not perform as well. The model appears to be somewhat underfit and does not reduce the false negative rate, 10.6% versus 7.5% for the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model3b, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth Model\n",
    "\n",
    "This model adds a 25% dropout layer and a dense layer with dimensionality of 128 to the first iteration of the third model. The model was compiled and fit in the same way as the `baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.Sequential()\n",
    "\n",
    "model4.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=3,\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(256,256,3)))\n",
    "\n",
    "model4.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=3,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model4.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\"))\n",
    "model4.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4.add(layers.Dropout(0.25))\n",
    "\n",
    "model4.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=3,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model4.add(layers.Conv2D(filters=8,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\"))\n",
    "model4.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4.add(layers.Flatten())\n",
    "\n",
    "model4.add(layers.Dense(128,activation=\"relu\"))\n",
    "model4.add(layers.Dense(64,activation=\"relu\"))\n",
    "model4.add(layers.Dense(32,activation=\"relu\"))\n",
    "model4.add(layers.Dense(16,activation=\"relu\"))\n",
    "model4.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model4.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model improved the accuracy for predicting pneumonia at the cost of a higher false negative rate. Overall, with false negative as our desired metric, this is our worst model (18.1% false negative rate). This model appears to be slightly underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model4, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth Model (Second Iteration)\n",
    "\n",
    "In order to try to capitalize on the increased accuracy in bacterial and viral pneumonia predictions, we will try to iterate on this model to reduce the false negative rate. Since the previous model appears to be slightly underfit, a early stop was defined in order to reduce the loss and get a better fit. This model was fit with 50 epochs and will early stop when noticable improvements are not made for 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4b = models.Sequential()\n",
    "\n",
    "model4b.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=3,\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(256,256,3)))\n",
    "\n",
    "model4b.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4b.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=3,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model4b.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\"))\n",
    "model4b.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4b.add(layers.Dropout(0.25))\n",
    "\n",
    "model4b.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=3,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model4b.add(layers.Conv2D(filters=8,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\"))\n",
    "model4b.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4b.add(layers.Flatten())\n",
    "\n",
    "model4b.add(layers.Dense(128,activation=\"relu\"))\n",
    "model4b.add(layers.Dense(64,activation=\"relu\"))\n",
    "model4b.add(layers.Dense(32,activation=\"relu\"))\n",
    "model4b.add(layers.Dense(16,activation=\"relu\"))\n",
    "model4b.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4b.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=1e-8,\n",
    "                           verbose=1,\n",
    "                           mode='min',\n",
    "                           patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model4b.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed better than the first iteration of model 4; however, the false negative rate is still not ideal at 12.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model4b, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth Model (Third Iteration)\n",
    "\n",
    "This iteration of the model removes the dropout layer. This will be the final iteration of the fourth model in order to optimize. This model utilizes the early stop from the second iteration and the same fitting as `model4b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4c = models.Sequential()\n",
    "\n",
    "model4c.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=3,\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(256,256,3)))\n",
    "\n",
    "model4c.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4c.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=3,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model4c.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\"))\n",
    "model4c.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4c.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=3,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "model4c.add(layers.Conv2D(filters=8,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\"))\n",
    "model4c.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "model4c.add(layers.Flatten())\n",
    "\n",
    "model4c.add(layers.Dense(128,activation=\"relu\"))\n",
    "model4c.add(layers.Dense(64,activation=\"relu\"))\n",
    "model4c.add(layers.Dense(32,activation=\"relu\"))\n",
    "model4c.add(layers.Dense(16,activation=\"relu\"))\n",
    "model4c.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4c.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model4c.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model does improve from the previous iterations, it appears to be slightly overfit and has a false negative rate of 11.9% despite having an accuracy of 79.8% when predicting bacterial pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model4c, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model\n",
    "\n",
    "The base third model performed the best and; therefor, was chosen for the final model. Initially, the model was run with early stopping; however, after evaluating those results, it appeared to be optimal with epochs set as 15. This is value that the final model was fit with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models.Sequential()\n",
    "\n",
    "final_model.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=3,\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(256,256,3)))\n",
    "\n",
    "final_model.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "final_model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "final_model.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation =\"relu\"))\n",
    "final_model.add(layers.MaxPooling2D(pool_size=2))\n",
    "final_model.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\",\n",
    "                        kernel_regularizer=regularizers.L2(l=0.05)))\n",
    "final_model.add(layers.Conv2D(filters=8,\n",
    "                        kernel_size=2,\n",
    "                        padding=\"same\",\n",
    "                        activation=\"relu\"))\n",
    "final_model.add(layers.MaxPooling2D(pool_size=2))\n",
    "\n",
    "final_model.add(layers.Flatten())\n",
    "\n",
    "final_model.add(layers.Dense(64,activation=\"relu\"))\n",
    "final_model.add(layers.Dense(32,activation=\"relu\"))\n",
    "final_model.add(layers.Dense(16,activation=\"relu\"))\n",
    "final_model.add(layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = final_model.fit(\n",
    "    train_images, \n",
    "    train_labels_encoded,\n",
    "    validation_data=(val_images, val_labels_encoded),\n",
    "    epochs=15,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final iteration of the model performed very well when predicting bacterial pneumonia at an accuracy of 79.9%. It also has the lowest rate for false negatives at just 7.4%. This model does not appear to be overfit and also appears to do a good job of minimizing the loss as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(final_model, results, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
